{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if in aws\n",
    "from io import StringIO\n",
    "from zipfile import Path\n",
    "zipped = Path(\"/home/ubuntu/work/BERT_Family/data/news/news.zip\", at=\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #for read data\n",
    "import numpy as np\n",
    "def load_data_to_demoData(data = None, sample_size = 2000):\n",
    "    sample_size = sample_size\n",
    "    demo_data = pd.DataFrame()\n",
    "    for i in pd.unique(data.label):\n",
    "        population = data[data.label == i].index\n",
    "        demo_data = demo_data.append(data.iloc[np.random.choice(population, sample_size, replace=False), :])\n",
    "    return demo_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建立Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "#demo_data = pd.DataFrame(load_dataset('glue', 'cola', split='train'))\n",
    "class MyDemoDataset(Dataset):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        #data = pd.read_csv(\"~/Desktop/train.csv\") #in mac\n",
    "        data = pd.read_csv(StringIO(zipped.read_text()))#in aws\n",
    "        demo_data = load_data_to_demoData(data=data)\n",
    "        self.x = demo_data[[\"title1_zh\", \"title1_zh\"]]\n",
    "        self.y = demo_data[\"label\"]\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, index):\n",
    "        #無意義處理，純粹示範用\n",
    "        result = len(self.x.iloc[index, 0])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建立Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2301/168849740.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  demo_data = demo_data.append(data.iloc[np.random.choice(population, sample_size, replace=False), :])\n",
      "/tmp/ipykernel_2301/168849740.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  demo_data = demo_data.append(data.iloc[np.random.choice(population, sample_size, replace=False), :])\n",
      "/tmp/ipykernel_2301/168849740.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  demo_data = demo_data.append(data.iloc[np.random.choice(population, sample_size, replace=False), :])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "myDataLoader = DataLoader(dataset=MyDemoDataset(), shuffle=True, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([21, 22, 28, 19, 27])\n",
      "tensor([24, 30, 11, 20, 22])\n",
      "tensor([29, 16, 17, 21, 24])\n",
      "tensor([30, 23, 22, 23, 27])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([30, 30, 21, 30, 30])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "for i, data in enumerate(myDataLoader):\n",
    "    print(data)\n",
    "    if i == 3:\n",
    "        break \n",
    "    \n",
    "demo_iter = iter(myDataLoader)\n",
    "next(demo_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers示範:\n",
    "Pre-trained Tokenizer from BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 4158, 749, 100, 5445, 4850, 5061, 4500, 4638, 1368, 2094, 8024, 4692, 4692, 6857, 1368, 100, 2527, 2768, 3126, 1963, 862, 511, 5439, 7962, 1469, 5439, 5988, 1004, 1004, 1146, 679, 3926, 3504, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, AlbertForSequenceClassification\n",
    "MY_PRETRAINED_MODEL = \"uer/albert-base-chinese-cluecorpussmall\"\n",
    "exmaple_text = \"為了DEMO而示範用的句子，看看這句Tokenize後成效如何。老鼠和老虎傻傻分不清楚\"\n",
    "tokenizer = BertTokenizer.from_pretrained(MY_PRETRAINED_MODEL)\n",
    "print(tokenizer(exmaple_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify __getitem__() on MyDemoDataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2301/168849740.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  demo_data = demo_data.append(data.iloc[np.random.choice(population, sample_size, replace=False), :])\n",
      "/tmp/ipykernel_2301/168849740.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  demo_data = demo_data.append(data.iloc[np.random.choice(population, sample_size, replace=False), :])\n",
      "/tmp/ipykernel_2301/168849740.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  demo_data = demo_data.append(data.iloc[np.random.choice(population, sample_size, replace=False), :])\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 101, 2582,  720,  ...,    0,    0,    0],\n",
      "        [ 101, 3330, 1352,  ...,    0,    0,    0],\n",
      "        [ 101, 4374, 2590,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101,  680, 6241,  ...,    0,    0,    0],\n",
      "        [ 101,  123, 3299,  ...,    0,    0,    0],\n",
      "        [ 101, 4220, 2111,  ...,    0,    0,    0]]), tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), tensor([2, 2, 2, 2, 1, 1, 1, 2, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 2, 2, 1, 2, 0, 0,\n",
      "        1, 1, 0, 1, 2, 0, 1, 0, 1, 0, 2, 0, 2, 2, 0, 0, 0, 1, 0, 1, 2, 0, 0, 0,\n",
      "        0, 2, 2, 1, 1, 0, 1, 1, 1, 2, 1, 1, 0, 1, 1, 2, 0, 0, 2, 0, 1, 1, 2, 2,\n",
      "        0, 0, 2, 0, 0, 2, 1, 1, 1, 1, 2, 1, 0, 2, 1, 0, 1, 0, 1, 0, 0, 1, 0, 2,\n",
      "        1, 2, 2, 2, 2, 0, 0, 2, 1, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 2, 0, 0,\n",
      "        2, 0, 1, 2, 0, 1, 0, 1])]\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 80\n",
    "class MyDemoDataset(Dataset):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        #data = pd.read_csv(\"~/Desktop/train.csv\") #in mac\n",
    "        data = pd.read_csv(StringIO(zipped.read_text()))#in aws\n",
    "        demo_data = load_data_to_demoData(data=data)\n",
    "        self.x = demo_data[[\"title1_zh\", \"title2_zh\"]]\n",
    "        self.y = demo_data[\"label\"]\n",
    "        self.y_dict = {ele:i for i, ele in enumerate(pd.unique(self.y))}\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(MY_PRETRAINED_MODEL)\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, index):\n",
    "        tokenize_result = self.tokenizer(self.x.iloc[index, 0], self.x.iloc[index, 1],\n",
    "                                         padding=\"max_length\", truncation=True, \n",
    "                                         max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "        input_ids, token_type_ids, attention_mask = map(torch.squeeze, tokenize_result.values())\n",
    "        mylabel =self.y.iloc[index,]\n",
    "        return input_ids, token_type_ids, attention_mask, self.y_dict[mylabel]\n",
    "\n",
    "#測試看看如何\n",
    "myDataLoader = DataLoader(dataset=MyDemoDataset(), shuffle=True, batch_size=128, pin_memory=True)\n",
    "print(next(iter(myDataLoader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New a pre-trained model from transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at uer/albert-base-chinese-cluecorpussmall were not used when initializing AlbertForSequenceClassification: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at uer/albert-base-chinese-cluecorpussmall and are newly initialized: ['classifier.weight', 'albert.pooler.bias', 'albert.pooler.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AlbertForSequenceClassification\n",
    "model = AlbertForSequenceClassification.from_pretrained(MY_PRETRAINED_MODEL, num_labels=3)\n",
    "myTrainData = next(iter(myDataLoader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10550275"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#configurations\n",
    "model.config\n",
    "\n",
    "#number of parameters\n",
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlbertForSequenceClassification(\n",
       "  (albert): AlbertModel(\n",
       "    (embeddings): AlbertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (encoder): AlbertTransformer(\n",
       "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
       "      (albert_layer_groups): ModuleList(\n",
       "        (0): AlbertLayerGroup(\n",
       "          (albert_layers): ModuleList(\n",
       "            (0): AlbertLayer(\n",
       "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (attention): AlbertAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (attention_dropout): Dropout(p=0, inplace=False)\n",
       "                (output_dropout): Dropout(p=0, inplace=False)\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (activation): ReLU()\n",
       "              (dropout): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (pooler_activation): Tanh()\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 768])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myState = model.state_dict()\n",
    "myState.keys()\n",
    "myState[\"albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value.weight\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "toy example train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10343/3075183845.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask=myTrainData[2][:toy_example_size, :], labels=torch.tensor(myTrainData[3][:toy_example_size]),\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(1.1592, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0473, -0.1408,  0.1968]], grad_fn=<AddmmBackward0>), hidden_states=(tensor([[[-0.2866, -0.1826,  0.1002,  ...,  0.5232,  0.5819,  0.2270],\n",
       "         [ 0.6979,  0.6134,  0.4206,  ...,  0.3448,  0.4538,  1.0133],\n",
       "         [ 0.0878,  0.2886, -0.8440,  ...,  0.4203,  0.5559, -0.1841],\n",
       "         ...,\n",
       "         [-0.2562, -0.7522, -0.3735,  ...,  0.2546, -0.2844, -0.8386],\n",
       "         [-0.1678, -0.6897, -0.3512,  ...,  0.4041, -0.4581, -0.8463],\n",
       "         [-0.1055, -0.5931, -0.3233,  ...,  0.4730, -0.5742, -0.8570]]],\n",
       "       grad_fn=<ViewBackward0>), tensor([[[ 0.0056, -0.2093,  0.2920,  ..., -0.2183,  0.1212, -0.2655],\n",
       "         [ 0.5546, -0.2476, -0.8361,  ..., -0.2857,  0.5083, -0.4056],\n",
       "         [ 0.3562,  0.6356, -1.8299,  ..., -0.1445,  0.8169, -0.8801],\n",
       "         ...,\n",
       "         [ 0.2381,  0.2537, -0.4637,  ...,  0.6134, -0.1090, -0.1703],\n",
       "         [ 0.2353,  0.2641, -0.3853,  ...,  0.5496, -0.1490, -0.1591],\n",
       "         [ 0.2355,  0.3591, -0.3783,  ...,  0.3398, -0.0734, -0.1059]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.3131, -0.1686,  0.3826,  ...,  0.1107, -0.4895, -0.5196],\n",
       "         [ 0.6029, -0.2464, -1.0837,  ...,  0.3522, -0.0704, -0.2792],\n",
       "         [ 0.7302,  0.6124, -1.5418,  ...,  0.3698,  0.2737, -0.8316],\n",
       "         ...,\n",
       "         [ 0.2117,  0.2041, -0.4087,  ..., -0.3614, -0.0186, -0.0219],\n",
       "         [ 0.1924,  0.1946, -0.4357,  ..., -0.5095,  0.0463, -0.0112],\n",
       "         [ 0.1490,  0.1511, -0.4827,  ..., -0.6774,  0.1993,  0.0038]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.4167, -0.1507,  0.5061,  ..., -0.2496, -0.6506, -0.4574],\n",
       "         [ 0.4865,  0.0521, -1.1666,  ...,  0.4048, -0.1481, -0.0341],\n",
       "         [ 0.6304,  0.5811, -1.0481,  ...,  0.2765,  0.4156, -1.0295],\n",
       "         ...,\n",
       "         [ 0.1940, -0.0216, -0.6671,  ..., -0.5774,  0.2068,  0.0189],\n",
       "         [ 0.1885, -0.0832, -0.7037,  ..., -0.6140,  0.2418, -0.0107],\n",
       "         [ 0.2208, -0.1499, -0.7644,  ..., -0.5754,  0.2482, -0.0591]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.7023,  0.2382,  0.4312,  ..., -0.8299, -0.5858, -0.3975],\n",
       "         [ 0.2546,  0.4972, -1.2232,  ...,  0.1180, -0.0229,  0.4591],\n",
       "         [ 0.5475,  0.2823, -1.1452,  ..., -0.2192,  0.8593, -0.6855],\n",
       "         ...,\n",
       "         [ 0.4655, -0.2329, -1.1726,  ..., -0.4244, -0.0600,  0.2715],\n",
       "         [ 0.5108, -0.2334, -1.1560,  ..., -0.3396, -0.1193,  0.2294],\n",
       "         [ 0.6039, -0.2097, -1.1406,  ..., -0.1930, -0.2360,  0.1988]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.7291,  0.5020,  0.5005,  ..., -1.1776, -0.5312, -0.3551],\n",
       "         [-0.0034,  0.1089, -1.0383,  ...,  0.1416, -0.0176,  0.2316],\n",
       "         [ 0.5607, -0.0198, -1.0065,  ...,  0.0344,  0.6556, -0.5923],\n",
       "         ...,\n",
       "         [ 0.7443, -0.2813, -0.8800,  ...,  0.0724, -0.5430,  0.4780],\n",
       "         [ 0.7502, -0.2210, -0.8704,  ...,  0.0986, -0.5832,  0.4846],\n",
       "         [ 0.7639, -0.1408, -0.8205,  ...,  0.1128, -0.5782,  0.5040]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 7.3861e-01,  5.6081e-01,  4.7558e-01,  ..., -1.2165e+00,\n",
       "          -3.1575e-01, -1.6678e-01],\n",
       "         [ 1.0333e-01, -2.9403e-02, -7.7620e-01,  ...,  7.0655e-01,\n",
       "          -2.3243e-01, -4.3353e-02],\n",
       "         [ 7.6782e-01,  8.3239e-02, -8.6188e-01,  ...,  3.8094e-01,\n",
       "           1.9436e-01, -4.5335e-01],\n",
       "         ...,\n",
       "         [ 6.5549e-01, -1.6256e-02, -5.2882e-01,  ...,  8.8390e-02,\n",
       "          -7.1326e-01,  7.2910e-01],\n",
       "         [ 6.2917e-01,  9.1944e-04, -5.5842e-01,  ...,  1.3377e-02,\n",
       "          -6.5743e-01,  7.4583e-01],\n",
       "         [ 5.7607e-01,  1.6411e-02, -6.4385e-01,  ..., -9.8057e-02,\n",
       "          -5.7510e-01,  7.9655e-01]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.6032,  0.4652,  0.5592,  ..., -1.3284, -0.1118, -0.0124],\n",
       "         [ 0.1760,  0.3444, -0.3633,  ...,  0.9771, -0.2635,  0.0733],\n",
       "         [ 0.5517,  0.5817, -0.7808,  ...,  0.5694, -0.2688, -0.2128],\n",
       "         ...,\n",
       "         [ 0.3436,  0.0899, -0.4724,  ..., -0.1174, -0.4514,  0.9604],\n",
       "         [ 0.2965,  0.0213, -0.4968,  ..., -0.1769, -0.3867,  0.9462],\n",
       "         [ 0.2377, -0.0794, -0.5538,  ..., -0.2276, -0.2970,  0.9340]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.4837,  0.3322,  0.5469,  ..., -1.3279,  0.0466,  0.0289],\n",
       "         [ 0.0508,  0.4582, -0.1503,  ...,  0.6132,  0.2457,  0.1440],\n",
       "         [ 0.3642,  0.7282, -0.9442,  ...,  0.2167, -0.2159, -0.0223],\n",
       "         ...,\n",
       "         [-0.0163, -0.1236, -0.4283,  ...,  0.0088, -0.2581,  0.9099],\n",
       "         [ 0.0050, -0.1678, -0.4304,  ...,  0.0379, -0.2943,  0.8807],\n",
       "         [ 0.0472, -0.1967, -0.4482,  ...,  0.1115, -0.3504,  0.8489]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.3797,  0.4186,  0.5718,  ..., -1.3163,  0.1028,  0.0218],\n",
       "         [-0.2147,  0.1952, -0.1329,  ...,  0.2686,  0.6304, -0.0233],\n",
       "         [-0.0467,  0.4246, -0.4934,  ...,  0.0801, -0.0942, -0.2806],\n",
       "         ...,\n",
       "         [-0.4072,  0.1709, -0.3012,  ...,  0.2542, -0.4600,  0.8246],\n",
       "         [-0.3755,  0.1957, -0.2733,  ...,  0.2997, -0.5147,  0.8188],\n",
       "         [-0.3399,  0.2250, -0.2579,  ...,  0.3574, -0.5703,  0.8322]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.2805,  0.5546,  0.6999,  ..., -1.2670,  0.3429, -0.0517],\n",
       "         [-0.4424, -0.0398,  0.0528,  ...,  0.1810,  0.5184, -0.5198],\n",
       "         [-0.6479,  0.3815, -0.0385,  ...,  0.3030, -0.2928, -0.8666],\n",
       "         ...,\n",
       "         [-0.8497,  0.6217,  0.0465,  ...,  0.1651, -0.3409,  0.5960],\n",
       "         [-0.8355,  0.6517,  0.0470,  ...,  0.1468, -0.3439,  0.6059],\n",
       "         [-0.8260,  0.6829,  0.0298,  ...,  0.1130, -0.3547,  0.6235]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 4.3355e-01,  7.4968e-01,  7.9436e-01,  ..., -1.3406e+00,\n",
       "           9.3288e-01,  2.5579e-01],\n",
       "         [-3.1194e-01, -5.4767e-01,  2.8473e-01,  ...,  7.4394e-02,\n",
       "           2.2361e-01, -6.9727e-01],\n",
       "         [-9.3353e-01,  4.8534e-01, -3.8534e-02,  ...,  4.2954e-02,\n",
       "          -6.1073e-01, -4.4105e-01],\n",
       "         ...,\n",
       "         [-9.0117e-01,  3.7905e-01,  1.4219e-01,  ..., -3.6947e-01,\n",
       "          -2.5669e-02,  8.1995e-01],\n",
       "         [-9.1759e-01,  3.6542e-01,  1.2875e-01,  ..., -4.0770e-01,\n",
       "          -7.1799e-03,  8.3211e-01],\n",
       "         [-9.4974e-01,  3.2790e-01,  1.1721e-01,  ..., -4.5385e-01,\n",
       "           8.5518e-04,  8.3710e-01]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[ 6.2027e-01,  4.8339e-01,  7.7677e-01,  ..., -1.5472e+00,\n",
       "           1.3949e+00,  4.5205e-01],\n",
       "         [-1.6695e-01, -6.3511e-01,  2.2653e-01,  ..., -8.8482e-02,\n",
       "           4.7098e-01, -7.7201e-01],\n",
       "         [-1.0194e+00,  2.2943e-01, -4.4153e-01,  ...,  3.3598e-02,\n",
       "          -7.9369e-01,  9.6490e-02],\n",
       "         ...,\n",
       "         [-1.3467e+00,  5.9438e-02, -3.5183e-01,  ..., -3.3801e-01,\n",
       "          -9.1252e-04,  7.4776e-01],\n",
       "         [-1.3553e+00,  2.3100e-02, -3.6135e-01,  ..., -3.4277e-01,\n",
       "           7.2252e-03,  7.4430e-01],\n",
       "         [-1.3765e+00, -2.2510e-02, -3.6050e-01,  ..., -3.5155e-01,\n",
       "           5.5285e-03,  7.3717e-01]]], grad_fn=<NativeLayerNormBackward0>)), attentions=None)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_example_size = 1 #只train 1 筆\n",
    "model.train()\n",
    "output = model(input_ids=myTrainData[0][:toy_example_size, :], token_type_ids=myTrainData[1][:toy_example_size, :], \n",
    "               attention_mask=myTrainData[2][:toy_example_size, :], labels=torch.tensor(myTrainData[3][:toy_example_size]), \n",
    "               output_hidden_states=True)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.cli import tqdm\n",
    "def train(model, dataloader, num_epochs):\n",
    "    base_lr = 1e-5\n",
    "    optimizer = torch.optim.Adam([{\"params\": model.albert.embeddings.parameters(), \"lr\": base_lr},\n",
    "                                  {\"params\": model.albert.encoder.parameters(),\n",
    "                                   \"lr\": base_lr/2}\n",
    "                                  ], lr=base_lr)\n",
    "    learning_schedule = torch.optim.lr_scheduler.StepLR(optimizer=myoptimizer, step_size=3, gamma=0.1)\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    auc, total = 0, 0\n",
    "    print(\"Start train...\")\n",
    "    for e in range(num_epochs):\n",
    "        running_loss = 0\n",
    "        for data in tqdm(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            labels = torch.tensor(data[3], device=device)\n",
    "            output = model(input_ids=data[0].to(device),\n",
    "                           token_type_ids=data[1].to(device),\n",
    "                           attention_mask=data[2].to(device),\n",
    "                           labels=labels,\n",
    "                           output_hidden_states=False)\n",
    "            output[0].backward()\n",
    "            running_loss += output[0].item()\n",
    "            optimizer.step()\n",
    "\n",
    "            auc += sum(labels == torch.max(output[1], 1).indices)\n",
    "            total += len(labels)\n",
    "        print(\"[Epochs]: \", e+1, \", AUC: \", auc /\n",
    "              total, \", loss: \", running_loss)\n",
    "        learning_schedule.step()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partial Layer Transfer (Fix前幾層 embedding的部分)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def change_layer_weights_require(model, layer_name:list=None, sign=False):\n",
    "    for layer in layer_name:\n",
    "        eval(\"model.\" + layer).requires_grad = sign\n",
    "    return model\n",
    "\n",
    "model = AlbertForSequenceClassification.from_pretrained(MY_PRETRAINED_MODEL, num_labels=3)\n",
    "all_layer_name = list(model.state_dict().keys())\n",
    "embeddings_filter = re.compile(\".*\\.embeddings\\.\")\n",
    "fine_tune_layer_name = list(filter(embeddings_filter.match, all_layer_name))\n",
    "print(fine_tune_layer_name)\n",
    "\n",
    "model_fix_front_layer = change_layer_weights_require(model, fine_tune_layer_name[1:], False)\n",
    "model_fix_front_layer = train(model=model_fix_front_layer, dataloader=myDataLoader)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partial Layer Transfer (fix後幾層）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AlbertForSequenceClassification.from_pretrained(MY_PRETRAINED_MODEL, num_labels=3)\n",
    "embeddings_filter = re.compile(\".*\\.encoder\\.\")\n",
    "fine_tune_layer_name = list(filter(embeddings_filter.match, all_layer_name))\n",
    "fine_tune_layer_name = list(map(lambda x:str.replace(x, \".0\", \"[0]\"), fine_tune_layer_name))\n",
    "print(fine_tune_layer_name)\n",
    "\n",
    "model_fix_back_layer = change_layer_weights_require(model, fine_tune_layer_name[1:], False)\n",
    "model_fix_back_layer = train(model=model_fix_back_layer, dataloader=myDataLoader)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Layer Transfer (fine-tune full model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AlbertForSequenceClassification.from_pretrained(MY_PRETRAINED_MODEL, num_labels=3)\n",
    "model = train(model=model, dataloader=myDataLoader, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#亂改\n",
    "tmp = torch.nn.Sequential(\n",
    "    model.pooler,\n",
    "    model.pooler_activation\n",
    ")\n",
    "myModel = torch.nn.ModuleDict({\n",
    "    \"a\": model.embeddings,\n",
    "    \"hahaha\": torch.nn.Linear(128,128),\n",
    "    \"b\": model.encoder,\n",
    "    \"c\": torch.nn.Linear(768, 768),\n",
    "    \"pooler\": tmp\n",
    "})\n",
    "output = model(input_ids=myTrainData[0], token_type_ids=myTrainData[1], \n",
    "               attention_mask=myTrainData[2], output_hidden_states=True)\n",
    "len(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
