{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if in aws\n",
    "from io import StringIO\n",
    "from zipfile import Path\n",
    "zipped = Path(\"/home/ubuntu/work/BERT_Family/data/news/news.zip\", at=\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #for read data\n",
    "import numpy as np\n",
    "def load_data_to_demoData(data = None, sample_size = 2000):\n",
    "    sample_size = sample_size\n",
    "    demo_data = pd.DataFrame()\n",
    "    for i in pd.unique(data.label):\n",
    "        population = data[data.label == i].index\n",
    "        demo_data = demo_data.append(data.iloc[np.random.choice(population, sample_size, replace=False), :])\n",
    "    return demo_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建立Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "#demo_data = pd.DataFrame(load_dataset('glue', 'cola', split='train'))\n",
    "class MyDemoDataset(Dataset):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        #data = pd.read_csv(\"~/Desktop/train.csv\") #in mac\n",
    "        data = pd.read_csv(StringIO(zipped.read_text()))#in aws\n",
    "        demo_data = load_data_to_demoData(data=data)\n",
    "        self.x = demo_data[[\"title1_zh\", \"title1_zh\"]]\n",
    "        self.y = demo_data[\"label\"]\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, index):\n",
    "        #無意義處理，純粹示範用\n",
    "        result = len(self.x.iloc[index, 0])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建立Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10343/168849740.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  demo_data = demo_data.append(data.iloc[np.random.choice(population, sample_size, replace=False), :])\n",
      "/tmp/ipykernel_10343/168849740.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  demo_data = demo_data.append(data.iloc[np.random.choice(population, sample_size, replace=False), :])\n",
      "/tmp/ipykernel_10343/168849740.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  demo_data = demo_data.append(data.iloc[np.random.choice(population, sample_size, replace=False), :])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "myDataLoader = DataLoader(dataset=MyDemoDataset(), shuffle=True, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([21, 22, 28, 19, 27])\n",
      "tensor([24, 30, 11, 20, 22])\n",
      "tensor([29, 16, 17, 21, 24])\n",
      "tensor([30, 23, 22, 23, 27])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([30, 30, 21, 30, 30])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "for i, data in enumerate(myDataLoader):\n",
    "    print(data)\n",
    "    if i == 3:\n",
    "        break \n",
    "    \n",
    "demo_iter = iter(myDataLoader)\n",
    "next(demo_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers示範:\n",
    "Pre-trained Tokenizer from BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 4158, 749, 100, 5445, 4850, 5061, 4500, 4638, 1368, 2094, 8024, 4692, 4692, 6857, 1368, 100, 2527, 2768, 3126, 1963, 862, 511, 5439, 7962, 1469, 5439, 5988, 1004, 1004, 1146, 679, 3926, 3504, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, AlbertForSequenceClassification\n",
    "MY_PRETRAINED_MODEL = \"uer/albert-base-chinese-cluecorpussmall\"\n",
    "exmaple_text = \"為了DEMO而示範用的句子，看看這句Tokenize後成效如何。老鼠和老虎傻傻分不清楚\"\n",
    "tokenizer = BertTokenizer.from_pretrained(MY_PRETRAINED_MODEL)\n",
    "print(tokenizer(exmaple_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify __getitem__() on MyDemoDataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10343/168849740.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  demo_data = demo_data.append(data.iloc[np.random.choice(population, sample_size, replace=False), :])\n",
      "/tmp/ipykernel_10343/168849740.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  demo_data = demo_data.append(data.iloc[np.random.choice(population, sample_size, replace=False), :])\n",
      "/tmp/ipykernel_10343/168849740.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  demo_data = demo_data.append(data.iloc[np.random.choice(population, sample_size, replace=False), :])\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 101, 2454, 1395,  ...,    0,    0,    0],\n",
      "        [ 101, 1065, 2336,  ...,    0,    0,    0],\n",
      "        [ 101, 2918, 4500,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 3250, 4494,  ...,    0,    0,    0],\n",
      "        [ 101, 2791, 7313,  ...,    0,    0,    0],\n",
      "        [ 101, 8468, 3165,  ...,    0,    0,    0]]), tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), tensor([2, 2, 2, 0, 1, 2, 0, 2, 0, 0, 2, 0, 1, 1, 0, 0, 0, 1, 1, 0, 2, 0, 0, 1,\n",
      "        1, 1, 2, 2, 1, 2, 1, 2, 2, 2, 2, 1, 2, 0, 1, 0, 1, 0, 2, 0, 0, 0, 1, 1,\n",
      "        1, 0, 2, 2, 2, 0, 2, 2, 1, 0, 1, 2, 0, 0, 0, 2, 2, 1, 0, 1, 1, 1, 1, 2,\n",
      "        2, 2, 1, 1, 1, 0, 2, 2, 2, 0, 1, 2, 2, 1, 2, 2, 2, 1, 2, 0, 1, 1, 1, 2,\n",
      "        2, 0, 2, 1, 0, 2, 2, 2, 0, 2, 1, 2, 0, 1, 0, 2, 2, 1, 1, 1, 0, 0, 2, 1,\n",
      "        0, 0, 2, 0, 2, 2, 1, 0])]\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 80\n",
    "class MyDemoDataset(Dataset):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        #data = pd.read_csv(\"~/Desktop/train.csv\") #in mac\n",
    "        data = pd.read_csv(StringIO(zipped.read_text()))#in aws\n",
    "        demo_data = load_data_to_demoData(data=data)\n",
    "        self.x = demo_data[[\"title1_zh\", \"title2_zh\"]]\n",
    "        self.y = demo_data[\"label\"]\n",
    "        self.y_dict = {ele:i for i, ele in enumerate(pd.unique(self.y))}\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(MY_PRETRAINED_MODEL)\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, index):\n",
    "        tokenize_result = self.tokenizer(self.x.iloc[index, 0], self.x.iloc[index, 1],\n",
    "                                         padding=\"max_length\", truncation=True, \n",
    "                                         max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "        input_ids, token_type_ids, attention_mask = map(torch.squeeze, tokenize_result.values())\n",
    "        mylabel =self.y.iloc[index,]\n",
    "        return input_ids, token_type_ids, attention_mask, self.y_dict[mylabel]\n",
    "\n",
    "#測試看看如何\n",
    "myDataLoader = DataLoader(dataset=MyDemoDataset(), shuffle=True, batch_size=128, pin_memory=True)\n",
    "print(next(iter(myDataLoader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New a pre-trained model from transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at uer/albert-base-chinese-cluecorpussmall were not used when initializing AlbertForSequenceClassification: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at uer/albert-base-chinese-cluecorpussmall and are newly initialized: ['albert.pooler.bias', 'classifier.bias', 'albert.pooler.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AlbertForSequenceClassification\n",
    "model = AlbertForSequenceClassification.from_pretrained(MY_PRETRAINED_MODEL, num_labels=3)\n",
    "myTrainData = next(iter(myDataLoader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10550275"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#configurations\n",
    "model.config\n",
    "\n",
    "#number of parameters\n",
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlbertForSequenceClassification(\n",
       "  (albert): AlbertModel(\n",
       "    (embeddings): AlbertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (encoder): AlbertTransformer(\n",
       "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
       "      (albert_layer_groups): ModuleList(\n",
       "        (0): AlbertLayerGroup(\n",
       "          (albert_layers): ModuleList(\n",
       "            (0): AlbertLayer(\n",
       "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (attention): AlbertAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (attention_dropout): Dropout(p=0, inplace=False)\n",
       "                (output_dropout): Dropout(p=0, inplace=False)\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (activation): ReLU()\n",
       "              (dropout): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (pooler_activation): Tanh()\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 768])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myState = model.state_dict()\n",
    "myState.keys()\n",
    "myState[\"albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value.weight\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "toy example train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10343/3075183845.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask=myTrainData[2][:toy_example_size, :], labels=torch.tensor(myTrainData[3][:toy_example_size]),\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(1.1592, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0473, -0.1408,  0.1968]], grad_fn=<AddmmBackward0>), hidden_states=(tensor([[[-0.2866, -0.1826,  0.1002,  ...,  0.5232,  0.5819,  0.2270],\n",
       "         [ 0.6979,  0.6134,  0.4206,  ...,  0.3448,  0.4538,  1.0133],\n",
       "         [ 0.0878,  0.2886, -0.8440,  ...,  0.4203,  0.5559, -0.1841],\n",
       "         ...,\n",
       "         [-0.2562, -0.7522, -0.3735,  ...,  0.2546, -0.2844, -0.8386],\n",
       "         [-0.1678, -0.6897, -0.3512,  ...,  0.4041, -0.4581, -0.8463],\n",
       "         [-0.1055, -0.5931, -0.3233,  ...,  0.4730, -0.5742, -0.8570]]],\n",
       "       grad_fn=<ViewBackward0>), tensor([[[ 0.0056, -0.2093,  0.2920,  ..., -0.2183,  0.1212, -0.2655],\n",
       "         [ 0.5546, -0.2476, -0.8361,  ..., -0.2857,  0.5083, -0.4056],\n",
       "         [ 0.3562,  0.6356, -1.8299,  ..., -0.1445,  0.8169, -0.8801],\n",
       "         ...,\n",
       "         [ 0.2381,  0.2537, -0.4637,  ...,  0.6134, -0.1090, -0.1703],\n",
       "         [ 0.2353,  0.2641, -0.3853,  ...,  0.5496, -0.1490, -0.1591],\n",
       "         [ 0.2355,  0.3591, -0.3783,  ...,  0.3398, -0.0734, -0.1059]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.3131, -0.1686,  0.3826,  ...,  0.1107, -0.4895, -0.5196],\n",
       "         [ 0.6029, -0.2464, -1.0837,  ...,  0.3522, -0.0704, -0.2792],\n",
       "         [ 0.7302,  0.6124, -1.5418,  ...,  0.3698,  0.2737, -0.8316],\n",
       "         ...,\n",
       "         [ 0.2117,  0.2041, -0.4087,  ..., -0.3614, -0.0186, -0.0219],\n",
       "         [ 0.1924,  0.1946, -0.4357,  ..., -0.5095,  0.0463, -0.0112],\n",
       "         [ 0.1490,  0.1511, -0.4827,  ..., -0.6774,  0.1993,  0.0038]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.4167, -0.1507,  0.5061,  ..., -0.2496, -0.6506, -0.4574],\n",
       "         [ 0.4865,  0.0521, -1.1666,  ...,  0.4048, -0.1481, -0.0341],\n",
       "         [ 0.6304,  0.5811, -1.0481,  ...,  0.2765,  0.4156, -1.0295],\n",
       "         ...,\n",
       "         [ 0.1940, -0.0216, -0.6671,  ..., -0.5774,  0.2068,  0.0189],\n",
       "         [ 0.1885, -0.0832, -0.7037,  ..., -0.6140,  0.2418, -0.0107],\n",
       "         [ 0.2208, -0.1499, -0.7644,  ..., -0.5754,  0.2482, -0.0591]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.7023,  0.2382,  0.4312,  ..., -0.8299, -0.5858, -0.3975],\n",
       "         [ 0.2546,  0.4972, -1.2232,  ...,  0.1180, -0.0229,  0.4591],\n",
       "         [ 0.5475,  0.2823, -1.1452,  ..., -0.2192,  0.8593, -0.6855],\n",
       "         ...,\n",
       "         [ 0.4655, -0.2329, -1.1726,  ..., -0.4244, -0.0600,  0.2715],\n",
       "         [ 0.5108, -0.2334, -1.1560,  ..., -0.3396, -0.1193,  0.2294],\n",
       "         [ 0.6039, -0.2097, -1.1406,  ..., -0.1930, -0.2360,  0.1988]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.7291,  0.5020,  0.5005,  ..., -1.1776, -0.5312, -0.3551],\n",
       "         [-0.0034,  0.1089, -1.0383,  ...,  0.1416, -0.0176,  0.2316],\n",
       "         [ 0.5607, -0.0198, -1.0065,  ...,  0.0344,  0.6556, -0.5923],\n",
       "         ...,\n",
       "         [ 0.7443, -0.2813, -0.8800,  ...,  0.0724, -0.5430,  0.4780],\n",
       "         [ 0.7502, -0.2210, -0.8704,  ...,  0.0986, -0.5832,  0.4846],\n",
       "         [ 0.7639, -0.1408, -0.8205,  ...,  0.1128, -0.5782,  0.5040]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 7.3861e-01,  5.6081e-01,  4.7558e-01,  ..., -1.2165e+00,\n",
       "          -3.1575e-01, -1.6678e-01],\n",
       "         [ 1.0333e-01, -2.9403e-02, -7.7620e-01,  ...,  7.0655e-01,\n",
       "          -2.3243e-01, -4.3353e-02],\n",
       "         [ 7.6782e-01,  8.3239e-02, -8.6188e-01,  ...,  3.8094e-01,\n",
       "           1.9436e-01, -4.5335e-01],\n",
       "         ...,\n",
       "         [ 6.5549e-01, -1.6256e-02, -5.2882e-01,  ...,  8.8390e-02,\n",
       "          -7.1326e-01,  7.2910e-01],\n",
       "         [ 6.2917e-01,  9.1944e-04, -5.5842e-01,  ...,  1.3377e-02,\n",
       "          -6.5743e-01,  7.4583e-01],\n",
       "         [ 5.7607e-01,  1.6411e-02, -6.4385e-01,  ..., -9.8057e-02,\n",
       "          -5.7510e-01,  7.9655e-01]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.6032,  0.4652,  0.5592,  ..., -1.3284, -0.1118, -0.0124],\n",
       "         [ 0.1760,  0.3444, -0.3633,  ...,  0.9771, -0.2635,  0.0733],\n",
       "         [ 0.5517,  0.5817, -0.7808,  ...,  0.5694, -0.2688, -0.2128],\n",
       "         ...,\n",
       "         [ 0.3436,  0.0899, -0.4724,  ..., -0.1174, -0.4514,  0.9604],\n",
       "         [ 0.2965,  0.0213, -0.4968,  ..., -0.1769, -0.3867,  0.9462],\n",
       "         [ 0.2377, -0.0794, -0.5538,  ..., -0.2276, -0.2970,  0.9340]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.4837,  0.3322,  0.5469,  ..., -1.3279,  0.0466,  0.0289],\n",
       "         [ 0.0508,  0.4582, -0.1503,  ...,  0.6132,  0.2457,  0.1440],\n",
       "         [ 0.3642,  0.7282, -0.9442,  ...,  0.2167, -0.2159, -0.0223],\n",
       "         ...,\n",
       "         [-0.0163, -0.1236, -0.4283,  ...,  0.0088, -0.2581,  0.9099],\n",
       "         [ 0.0050, -0.1678, -0.4304,  ...,  0.0379, -0.2943,  0.8807],\n",
       "         [ 0.0472, -0.1967, -0.4482,  ...,  0.1115, -0.3504,  0.8489]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.3797,  0.4186,  0.5718,  ..., -1.3163,  0.1028,  0.0218],\n",
       "         [-0.2147,  0.1952, -0.1329,  ...,  0.2686,  0.6304, -0.0233],\n",
       "         [-0.0467,  0.4246, -0.4934,  ...,  0.0801, -0.0942, -0.2806],\n",
       "         ...,\n",
       "         [-0.4072,  0.1709, -0.3012,  ...,  0.2542, -0.4600,  0.8246],\n",
       "         [-0.3755,  0.1957, -0.2733,  ...,  0.2997, -0.5147,  0.8188],\n",
       "         [-0.3399,  0.2250, -0.2579,  ...,  0.3574, -0.5703,  0.8322]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.2805,  0.5546,  0.6999,  ..., -1.2670,  0.3429, -0.0517],\n",
       "         [-0.4424, -0.0398,  0.0528,  ...,  0.1810,  0.5184, -0.5198],\n",
       "         [-0.6479,  0.3815, -0.0385,  ...,  0.3030, -0.2928, -0.8666],\n",
       "         ...,\n",
       "         [-0.8497,  0.6217,  0.0465,  ...,  0.1651, -0.3409,  0.5960],\n",
       "         [-0.8355,  0.6517,  0.0470,  ...,  0.1468, -0.3439,  0.6059],\n",
       "         [-0.8260,  0.6829,  0.0298,  ...,  0.1130, -0.3547,  0.6235]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 4.3355e-01,  7.4968e-01,  7.9436e-01,  ..., -1.3406e+00,\n",
       "           9.3288e-01,  2.5579e-01],\n",
       "         [-3.1194e-01, -5.4767e-01,  2.8473e-01,  ...,  7.4394e-02,\n",
       "           2.2361e-01, -6.9727e-01],\n",
       "         [-9.3353e-01,  4.8534e-01, -3.8534e-02,  ...,  4.2954e-02,\n",
       "          -6.1073e-01, -4.4105e-01],\n",
       "         ...,\n",
       "         [-9.0117e-01,  3.7905e-01,  1.4219e-01,  ..., -3.6947e-01,\n",
       "          -2.5669e-02,  8.1995e-01],\n",
       "         [-9.1759e-01,  3.6542e-01,  1.2875e-01,  ..., -4.0770e-01,\n",
       "          -7.1799e-03,  8.3211e-01],\n",
       "         [-9.4974e-01,  3.2790e-01,  1.1721e-01,  ..., -4.5385e-01,\n",
       "           8.5518e-04,  8.3710e-01]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[ 6.2027e-01,  4.8339e-01,  7.7677e-01,  ..., -1.5472e+00,\n",
       "           1.3949e+00,  4.5205e-01],\n",
       "         [-1.6695e-01, -6.3511e-01,  2.2653e-01,  ..., -8.8482e-02,\n",
       "           4.7098e-01, -7.7201e-01],\n",
       "         [-1.0194e+00,  2.2943e-01, -4.4153e-01,  ...,  3.3598e-02,\n",
       "          -7.9369e-01,  9.6490e-02],\n",
       "         ...,\n",
       "         [-1.3467e+00,  5.9438e-02, -3.5183e-01,  ..., -3.3801e-01,\n",
       "          -9.1252e-04,  7.4776e-01],\n",
       "         [-1.3553e+00,  2.3100e-02, -3.6135e-01,  ..., -3.4277e-01,\n",
       "           7.2252e-03,  7.4430e-01],\n",
       "         [-1.3765e+00, -2.2510e-02, -3.6050e-01,  ..., -3.5155e-01,\n",
       "           5.5285e-03,  7.3717e-01]]], grad_fn=<NativeLayerNormBackward0>)), attentions=None)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_example_size = 1 #只train 1 筆\n",
    "model.train()\n",
    "output = model(input_ids=myTrainData[0][:toy_example_size, :], token_type_ids=myTrainData[1][:toy_example_size, :], \n",
    "               attention_mask=myTrainData[2][:toy_example_size, :], labels=torch.tensor(myTrainData[3][:toy_example_size]), \n",
    "               output_hidden_states=True)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at uer/albert-base-chinese-cluecorpussmall were not used when initializing AlbertForSequenceClassification: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at uer/albert-base-chinese-cluecorpussmall and are newly initialized: ['albert.pooler.bias', 'classifier.bias', 'albert.pooler.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/47 [00:00<?, ?it/s]/tmp/ipykernel_10343/1205599867.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(data[3], device=device)\n",
      "  2%|▏         | 1/47 [00:03<02:21,  3.07s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  6%|▋         | 3/47 [00:09<02:17,  3.12s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      " 17%|█▋        | 8/47 [00:24<02:00,  3.08s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      " 36%|███▌      | 17/47 [00:52<01:32,  3.09s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      " 38%|███▊      | 18/47 [00:55<01:29,  3.09s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      " 40%|████      | 19/47 [00:58<01:26,  3.09s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      " 57%|█████▋    | 27/47 [01:23<01:01,  3.09s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      " 68%|██████▊   | 32/47 [01:38<00:46,  3.09s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      " 77%|███████▋  | 36/47 [01:51<00:34,  3.09s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      " 83%|████████▎ | 39/47 [02:00<00:24,  3.09s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "100%|██████████| 47/47 [02:25<00:00,  3.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epochs]:  1 , AUC:  tensor(0.3490, device='cuda:0') , loss:  52.897786259651184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/47 [00:00<?, ?it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  2%|▏         | 1/47 [00:06<04:45,  6.20s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[149], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n\u001b[1;32m     37\u001b[0m model \u001b[39m=\u001b[39m AlbertForSequenceClassification\u001b[39m.\u001b[39mfrom_pretrained(MY_PRETRAINED_MODEL, num_labels\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m model \u001b[39m=\u001b[39m train(model\u001b[39m=\u001b[39;49mmodel, dataloader\u001b[39m=\u001b[39;49mmyDataLoader, learning_schedule\u001b[39m=\u001b[39;49mlearning_scheduler, num_epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, optimizer\u001b[39m=\u001b[39;49mmyoptimizer)  \n",
      "Cell \u001b[0;32mIn[149], line 27\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, learning_schedule, num_epochs)\u001b[0m\n\u001b[1;32m     21\u001b[0m labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(data[\u001b[39m3\u001b[39m], device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m     22\u001b[0m output \u001b[39m=\u001b[39m model(input_ids\u001b[39m=\u001b[39mdata[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(device),\n\u001b[1;32m     23\u001b[0m                token_type_ids\u001b[39m=\u001b[39mdata[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(device),\n\u001b[1;32m     24\u001b[0m                attention_mask\u001b[39m=\u001b[39mdata[\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mto(device),\n\u001b[1;32m     25\u001b[0m                labels\u001b[39m=\u001b[39mlabels,\n\u001b[1;32m     26\u001b[0m                output_hidden_states\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> 27\u001b[0m output[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     28\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m output[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     29\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.cli import tqdm\n",
    "base_lr = 1e-5\n",
    "myoptimizer = torch.optim.Adam([{\"params\": model.parameters()}],\n",
    "                              lr=base_lr)\n",
    "myoptimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "learning_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer=myoptimizer, step_size=3, gamma=0.1)\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def train(model, dataloader, optimizer, learning_schedule, num_epochs):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    auc, total = 0, 0\n",
    "    print(\"Start train...\")\n",
    "    for e in range(num_epochs):\n",
    "        running_loss = 0\n",
    "        for data in tqdm(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            labels = torch.tensor(data[3], device=device)\n",
    "            output = model(input_ids=data[0].to(device),\n",
    "                           token_type_ids=data[1].to(device),\n",
    "                           attention_mask=data[2].to(device),\n",
    "                           labels=labels,\n",
    "                           output_hidden_states=False)\n",
    "            output[0].backward()\n",
    "            running_loss += output[0].item()\n",
    "            optimizer.step()\n",
    "\n",
    "            auc += sum(labels == torch.max(output[1], 1).indices)\n",
    "            total += len(labels)\n",
    "        print(\"[Epochs]: \", e+1, \", AUC: \", auc /\n",
    "              total, \", loss: \", running_loss)\n",
    "        learning_schedule.step()\n",
    "    return model\n",
    "model = AlbertForSequenceClassification.from_pretrained(MY_PRETRAINED_MODEL, num_labels=3)\n",
    "model = train(model=model, dataloader=myDataLoader, learning_schedule=learning_scheduler, num_epochs=10, optimizer=myoptimizer)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "albert\n",
      "dropout\n",
      "classifier\n"
     ]
    }
   ],
   "source": [
    "for i, _ in model.named_children():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "albert\n",
      "dropout\n",
      "classifier\n"
     ]
    }
   ],
   "source": [
    "for i, _ in model.named_children():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6271"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partial Layer Transfer (Fix前幾層 embedding的部分)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def change_layer_weights_require(model, layer_name:list=None, sign=False):\n",
    "    for layer in layer_name:\n",
    "        eval(\"model.\" + layer).requires_grad = sign\n",
    "    return model\n",
    "\n",
    "model = AlbertForSequenceClassification.from_pretrained(MY_PRETRAINED_MODEL, num_labels=2)\n",
    "all_layer_name = list(model.state_dict().keys())\n",
    "embeddings_filter = re.compile(\".*\\.embeddings\\.\")\n",
    "fine_tune_layer_name = list(filter(embeddings_filter.match, all_layer_name))\n",
    "print(fine_tune_layer_name)\n",
    "\n",
    "model_fix_front_layer = change_layer_weights_require(model, fine_tune_layer_name[1:], False)\n",
    "model_fix_front_layer = train(model_fix_front_layer, myDataLoader, optimizer, learning_scheduler, num_epochs=10)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partial Layer Transfer (fix後幾層）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at uer/albert-base-chinese-cluecorpussmall were not used when initializing AlbertForSequenceClassification: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at uer/albert-base-chinese-cluecorpussmall and are newly initialized: ['albert.pooler.bias', 'classifier.bias', 'albert.pooler.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[136], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[39m=\u001b[39m AlbertForSequenceClassification\u001b[39m.\u001b[39mfrom_pretrained(MY_PRETRAINED_MODEL, num_labels\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m embeddings_filter \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39mcompile(\u001b[39m\"\u001b[39m\u001b[39m.*\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m.encoder\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m fine_tune_layer_name \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mfilter\u001b[39m(embeddings_filter\u001b[39m.\u001b[39mmatch, all_layer_name))\n\u001b[1;32m      4\u001b[0m fine_tune_layer_name \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m x:\u001b[39mstr\u001b[39m\u001b[39m.\u001b[39mreplace(x, \u001b[39m\"\u001b[39m\u001b[39m.0\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m[0]\u001b[39m\u001b[39m\"\u001b[39m), fine_tune_layer_name))\n",
      "\u001b[0;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "model = AlbertForSequenceClassification.from_pretrained(MY_PRETRAINED_MODEL, num_labels=2)\n",
    "embeddings_filter = re.compile(\".*\\.encoder\\.\")\n",
    "fine_tune_layer_name = list(filter(embeddings_filter.match, all_layer_name))\n",
    "fine_tune_layer_name = list(map(lambda x:str.replace(x, \".0\", \"[0]\"), fine_tune_layer_name))\n",
    "print(fine_tune_layer_name)\n",
    "\n",
    "model_fix_back_layer = change_layer_weights_require(model, fine_tune_layer_name[1:], False)\n",
    "model_fix_back_layer = train(model_fix_back_layer, myDataLoader, optimizer, learning_scheduler, num_epochs=10)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at uer/albert-base-chinese-cluecorpussmall were not used when initializing AlbertForSequenceClassification: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at uer/albert-base-chinese-cluecorpussmall and are newly initialized: ['albert.pooler.bias', 'classifier.bias', 'albert.pooler.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/47 [00:00<?, ?it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "/tmp/ipykernel_10343/4232237109.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(data[3], device=device)\n",
      " 19%|█▉        | 9/47 [00:27<01:57,  3.08s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      " 23%|██▎       | 11/47 [00:33<01:51,  3.09s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      " 26%|██▌       | 12/47 [00:37<01:48,  3.09s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      " 30%|██▉       | 14/47 [00:43<01:42,  3.09s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      " 36%|███▌      | 17/47 [00:52<01:32,  3.09s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      " 51%|█████     | 24/47 [01:14<01:11,  3.09s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      " 57%|█████▋    | 27/47 [01:23<01:01,  3.10s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      " 60%|█████▉    | 28/47 [01:26<00:58,  3.10s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      " 72%|███████▏  | 34/47 [01:45<00:40,  3.10s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "100%|██████████| 47/47 [02:25<00:00,  3.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epochs]:  1 , AUC:  tensor(0.3332, device='cuda:0') , loss:  53.07018435001373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/47 [00:03<02:22,  3.10s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  6%|▋         | 3/47 [00:09<02:16,  3.10s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      " 15%|█▍        | 7/47 [00:21<02:04,  3.10s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      " 19%|█▉        | 9/47 [00:27<01:57,  3.10s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      " 28%|██▊       | 13/47 [00:40<01:45,  3.11s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[140], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#optimizer = torch.optim.Adam(model.parameters(), 1e-05)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m model \u001b[39m=\u001b[39m AlbertForSequenceClassification\u001b[39m.\u001b[39mfrom_pretrained(MY_PRETRAINED_MODEL, num_labels\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m model \u001b[39m=\u001b[39m train(model, myDataLoader, optimizer, learning_scheduler, num_epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)  \n",
      "Cell \u001b[0;32mIn[139], line 18\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, learning_schedule, num_epochs)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m     17\u001b[0m     running_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> 18\u001b[0m     \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m tqdm(dataloader):\n\u001b[1;32m     19\u001b[0m         optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     20\u001b[0m         labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(data[\u001b[39m3\u001b[39m], device\u001b[39m=\u001b[39mdevice)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:61\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 61\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    205\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:143\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    140\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 143\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:143\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    140\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 143\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:120\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 120\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map)\n\u001b[1;32m    122\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    123\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:163\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    161\u001b[0m     storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mstorage()\u001b[39m.\u001b[39m_new_shared(numel, device\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    162\u001b[0m     out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[0;32m--> 163\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(batch, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#optimizer = torch.optim.Adam(model.parameters(), 1e-5)\n",
    "model = AlbertForSequenceClassification.from_pretrained(MY_PRETRAINED_MODEL, num_labels=3)\n",
    "model = train(model, myDataLoader, optimizer, learning_scheduler, num_epochs=10)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset glue/cola to /home/ubuntu/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.019474506378173828,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading data",
       "rate": null,
       "total": 376971,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7fa88c894bf431aa13dcc756ccdee76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/377k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015415191650390625,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Generating train split",
       "rate": null,
       "total": 8551,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79660dbe0c9f42238499b9dd15ed6cfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/8551 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015361785888671875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Generating validation split",
       "rate": null,
       "total": 1043,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a72a358f6fd44d83b9ef5396cb511d44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1043 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015399694442749023,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Generating test split",
       "rate": null,
       "total": 1063,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c082d10bbf6b457386ee2b108121845e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1063 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset glue downloaded and prepared to /home/ubuntu/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('glue', 'cola', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/home/ubuntu/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('glue', 'mrpc', split='train')\n",
    "\n",
    "from tqdm.cli  import tqdm\n",
    "base_lr = 0.005\n",
    "optimizer = torch.optim.Adam([{'params': model.albert.embeddings.parameters(), \"lr\": base_lr},\n",
    "                              {'params': model.albert.encoder.parameters(), \"lr\": base_lr/2}],\n",
    "                              lr=0.001)\n",
    "\n",
    "learning_scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=3, gamma=0.1)\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "def train(model, dataloader, optimizer, learning_schedule, num_epochs):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    auc, total = 0, 0\n",
    "    print(\"Start train...\")\n",
    "    for e in range(num_epochs):\n",
    "        running_loss = 0\n",
    "        for data in tqdm(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            labels = torch.tensor(data[3], device=device)\n",
    "            output = model(input_ids=data[0].to(device), \n",
    "                           token_type_ids=data[1].to(device), \n",
    "                           attention_mask=data[2].to(device), \n",
    "                           labels=labels, \n",
    "                           output_hidden_states=False)\n",
    "            output[0].backward()\n",
    "            running_loss += output[0]\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "            auc += sum(labels == torch.max(output[1], 1).indices)\n",
    "            total += len(labels)\n",
    "        print(\"[Epochs]: \", e+1, \", AUC: \", auc/total, \", loss: \", running_loss)\n",
    "        learning_schedule.step()\n",
    "    return model\n",
    "model = train(model, DataLoader(dataset, batch_size=128, pin_memory=True, shuffle=True),\n",
    "              optimizer, learning_scheduler, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/47 [00:00<?, ?it/s]/tmp/ipykernel_10343/459945426.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(data[3], device=device)\n",
      "  4%|▍         | 2/47 [00:06<02:24,  3.21s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  9%|▊         | 4/47 [00:12<02:14,  3.12s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      " 11%|█         | 5/47 [00:15<02:10,  3.11s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      " 13%|█▎        | 6/47 [00:18<02:06,  3.09s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      " 26%|██▌       | 12/47 [00:37<01:48,  3.09s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      " 34%|███▍      | 16/47 [00:49<01:35,  3.08s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      " 38%|███▊      | 18/47 [00:55<01:29,  3.08s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      " 45%|████▍     | 21/47 [01:05<01:20,  3.08s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      " 49%|████▉     | 23/47 [01:11<01:13,  3.08s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      " 51%|█████     | 24/47 [01:14<01:10,  3.08s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      " 62%|██████▏   | 29/47 [01:29<00:55,  3.07s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      " 66%|██████▌   | 31/47 [01:35<00:49,  3.09s/it]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m train(model, myDataLoader, optimizer, learning_scheduler, num_epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)  \n",
      "Cell \u001b[0;32mIn[9], line 16\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, learning_schedule, num_epochs)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m     15\u001b[0m     running_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> 16\u001b[0m     \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m tqdm(dataloader):\n\u001b[1;32m     17\u001b[0m         optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     18\u001b[0m         labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(data[\u001b[39m3\u001b[39m], device\u001b[39m=\u001b[39mdevice)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[7], line 16\u001b[0m, in \u001b[0;36mMyDemoDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[0;32m---> 16\u001b[0m     tokenize_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx\u001b[39m.\u001b[39;49miloc[index, \u001b[39m0\u001b[39;49m], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx\u001b[39m.\u001b[39;49miloc[index, \u001b[39m1\u001b[39;49m], \n\u001b[1;32m     17\u001b[0m                                      padding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmax_length\u001b[39;49m\u001b[39m\"\u001b[39;49m, truncation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \n\u001b[1;32m     18\u001b[0m                                      max_length\u001b[39m=\u001b[39;49mMAX_LENGTH, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     19\u001b[0m     input_ids, token_type_ids, attention_mask \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(torch\u001b[39m.\u001b[39msqueeze, tokenize_result\u001b[39m.\u001b[39mvalues())\n\u001b[1;32m     20\u001b[0m     mylabel \u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39my\u001b[39m.\u001b[39miloc[index,]\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2520\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2518\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2519\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2520\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_one(text\u001b[39m=\u001b[39;49mtext, text_pair\u001b[39m=\u001b[39;49mtext_pair, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mall_kwargs)\n\u001b[1;32m   2521\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2522\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2584\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2578\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2579\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2580\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2581\u001b[0m     )\n\u001b[1;32m   2583\u001b[0m \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_valid_text_input(text_pair):\n\u001b[0;32m-> 2584\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2585\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2586\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2587\u001b[0m     )\n\u001b[1;32m   2589\u001b[0m \u001b[39mif\u001b[39;00m is_split_into_words:\n\u001b[1;32m   2590\u001b[0m     is_batched \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(text, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)) \u001b[39mand\u001b[39;00m text \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(text[\u001b[39m0\u001b[39m], (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m))\n",
      "\u001b[0;31mValueError\u001b[0m: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "model = train(model, myDataLoader, optimizer, learning_scheduler, num_epochs=5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21023"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Layer Transfer (fine-tune full model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#亂改\n",
    "tmp = torch.nn.Sequential(\n",
    "    model.pooler,\n",
    "    model.pooler_activation\n",
    ")\n",
    "myModel = torch.nn.ModuleDict({\n",
    "    \"a\": model.embeddings,\n",
    "    \"hahaha\": torch.nn.Linear(128,128),\n",
    "    \"b\": model.encoder,\n",
    "    \"c\": torch.nn.Linear(768, 768),\n",
    "    \"pooler\": tmp\n",
    "})\n",
    "output = model(input_ids=myTrainData[0], token_type_ids=myTrainData[1], \n",
    "               attention_mask=myTrainData[2], output_hidden_states=True)\n",
    "len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "output = model(input_ids=myTrainData[0][:toy_example_size, :], token_type_ids=myTrainData[1][:toy_example_size, :], \n",
    "               attention_mask=myTrainData[2][:toy_example_size, :], labels=torch.tensor(myTrainData[3][:toy_example_size]), \n",
    "               output_hidden_states=True)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.num_hidden_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "test = BertModel.from_pretrained(\"bert-base-uncased\", num_labels=3)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.train()\n",
    "output = test(input_ids=myTrainData[0][:toy_example_size, :], token_type_ids=myTrainData[1][:toy_example_size, :], \n",
    "               attention_mask=myTrainData[2][:toy_example_size, :], \n",
    "               output_hidden_states=True)\n",
    "len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AlbertConfig\n",
    "\n",
    "myConfig = AlbertConfig(\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=17,\n",
    "    num_attention_heads=12,\n",
    "    hidden_act=\"silu\",\n",
    "    hidden_dropout_prob=0.1,\n",
    "    attention_probs_dropout_prob=0.2,\n",
    "    classifier_dropout_prob=0.3,\n",
    "    output_hidden_states=True,\n",
    "    output_attentions=True,\n",
    "    num_labels=3,\n",
    "    torchscript=True\n",
    ")\n",
    "AlbertForSequenceClassification.from_pretrained(\"uer/albert-base-chinese-cluecorpussmall\", config=myConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, AlbertModel, AlbertForSequenceClassification\n",
    "model = AlbertForSequenceClassification.from_pretrained(\"uer/albert-base-chinese-cluecorpussmall\", config=myConfig, ignore_mismatched_sizes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, AlbertModel, AlbertModel\n",
    "model = AlbertModel.from_pretrained(\"uer/albert-base-chinese-cluecorpussmall\", config=myConfig, ignore_mismatched_sizes=True)\n",
    "model.train()\n",
    "output = model(input_ids=myTrainData[0], token_type_ids=myTrainData[1], \n",
    "               attention_mask=myTrainData[2], output_hidden_states=True)\n",
    "len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#看每一層的參數\n",
    "m_s = model.state_dict()#dict type\n",
    "#看能夠叫誰\n",
    "m_s.keys()\n",
    "#ex\n",
    "m_s[\"encoder.albert_layer_groups.0.albert_layers.0.ffn_output.weight\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#查詢 model去看結構\n",
    "model.encoder\n",
    "#刪除\n",
    "del model.poolear\n",
    "#新增 只能在最後面\n",
    "model.add_module(\"name_yourself\", torch.nn.Linear(10, 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "del model.pooler, model.pooler_activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embeddings\n",
    "myModel = torch.nn.Sequential(\n",
    "    model.embeddings,\n",
    "    \"hahaha\": torch.nn.Linear(128,128),\n",
    "    model.encoder,\n",
    "    torch.nn.Linear(768, 768),\n",
    "    tmp\n",
    ")\n",
    "myModel\n",
    "output = model(input_ids=myTrainData[0], token_type_ids=myTrainData[1], \n",
    "               attention_mask=myTrainData[2], output_hidden_states=True)\n",
    "len(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
